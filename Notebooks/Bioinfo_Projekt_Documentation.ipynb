{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bioinfo Projekt Gruppe 4-4: k-means \n",
    "*Members: Benedict, Julia, Thorge and Marilena*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Implement the following algorithms in python using the numpy library:\n",
    "\n",
    "1. implement k-means:\n",
    "\n",
    "1. compare your implementation with the sklearn implementation with respect to quality and speed\n",
    "  implement mini-batch k-means:\n",
    "\n",
    "1. compare your implementation with the sklearn implementation with respect to quality and speed\n",
    "  implement k-means++ initialization:\n",
    "\n",
    "1. Compare the runtime and quality of your k-means implementation and your mini batch k-means implementation for  different datasets. You can use code from sklearn to generate datasets of arbitrary size and difficulty  (https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py). You should generate multiple plots to visualize the comparison (eg. you can plot the runtinme / cluster quality for different dataset sizes / number of clusters)\n",
    "\n",
    "1. Cluster the 3K PBMCs from a Healthy Donor Dataset from 10x Genomics\n",
    "1. use scanpy to load the data ( see https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)\n",
    "1. compare the performance of your implementations with the sklearn implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all a few packages are imported that were used throughout the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from statistics import mean\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request as url\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import csv\n",
    "from sklearn.base import BaseEstimator, ClusterMixin, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class Kmeans() is based on the sklearn variant.  \n",
    "It takes following arguments:  \n",
    "     **inits** --> initialisations  \n",
    "     Standard = 10    \n",
    "        **.k** --> number of clusters  \n",
    "        Standard = 8  \n",
    "    **maxit** --> maximum iterations.  \n",
    "    Standard = 300  \n",
    "    **method** --> method of choosing starting clusters.  \n",
    "    Standard = \"++\". Option = \"rng\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kmeans(BaseEstimator, ClusterMixin, TransformerMixin):               # Input: processed dataset, Output: clustered data (kmeans, kmeans++)\n",
    "    def __init__(self, inits=10, k=8, maxit=300, method=\"++\"):\n",
    "        \n",
    "        self.labels_ = None\n",
    "        self.cluster_centers_ = None\n",
    "        self._inits = inits\n",
    "        self._k = k\n",
    "        self._maxit = maxit\n",
    "        self._method = method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fit method:\n",
    "\n",
    "fits the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self,data):\n",
    "        self._data = data\n",
    "        best_clust = float('inf')\n",
    "        \n",
    "        for i in (range(self._inits)):\n",
    "            dot = np.random.choice(range(len(self._data)), self._k, replace=False)\n",
    "            self.cluster_centers_ = self._data[dot]\n",
    "            for i in range(self._maxit):\n",
    "                clusters = np.expand_dims(self.cluster_centers_, axis=1)\n",
    "                data = np.expand_dims(self._data, axis=0)\n",
    "                eucl = np.linalg.norm(clusters-data, axis=2) # euclidean dist by using integrated numpy function\n",
    "                self.labels_ = np.argmin(eucl, axis = 0)\n",
    "                for i in range(self._k): # range of clusters\n",
    "                    position = np.where(self.labels_ == i) # position im array bestimmen und dann die entspechenden punkte aus data auslesen\n",
    "                    self.cluster_centers_[i] = self._data[position].mean(axis = 0)\n",
    "                    #out = pd.DataFrame(data[np.argwhere(dist == i)].squeeze())\n",
    "                overall_quality = np.sum(np.min(eucl.T, axis=1))\n",
    "                if overall_quality < best_clust:\n",
    "                    best_clust = overall_quality\n",
    "                    best_dist = self.labels_\n",
    "                    best_centers = self.cluster_centers_\n",
    "            self.cluster_centers_ = best_centers\n",
    "            self.labels_ = best_dist\n",
    "                \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now expanded by kmeans++  \n",
    "set argument: method=\"++\" (by default as in sklearn)\n",
    "\n",
    "    Instead of all centroids only the first is appended by random choice.\n",
    "\n",
    "__The remaining k-1 centroids are chosen as followed:__ \n",
    "\n",
    "**1.** Calculate squared distance D of every point to its clostest centroid  \n",
    "**2.** Every point is assigned a probability to be chosen as the next centroid according to:\n",
    "\n",
    "$$D(x)\\over \\sum^{}_{x\\in X} D(x)$$  \n",
    "\n",
    "**3.** New centroid is picked from all datapoints considering their assigned probabilities  \n",
    "**4.** Repeat steps 1-3 until k centroids are chosen\n",
    "\n",
    "    Note: This method only provides intial centroids and does not change the clustering process for the following iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            if self._method == \"rng\": # random centers are choosen\n",
    "                #print(\"rng\")\n",
    "                dot = np.random.choice(range(len(self._data)), self._k, replace=False)\n",
    "                self.cluster_centers_ = self._data[dot]\n",
    "            elif self._method == \"++\": # kmeans++ is initiated\n",
    "                #print(\"++\")\n",
    "                dot = np.random.choice(len(self._data), replace=False) # random startpunkt\n",
    "                clusters = np.array([self._data[dot]])\n",
    "                pointer = np.array([])\n",
    "                for i in range (self._k-1):\n",
    "                    D = np.array([])\n",
    "            \n",
    "                    for j in range (len(self._data)):\n",
    "                        D = np.append(D,np.min(np.sum((self._data[j]-clusters)**2, axis = 1)))\n",
    "                \n",
    "                    pointer = np.append(pointer, D, axis = 0) \n",
    "            \n",
    "                    p = D/np.sum(D)\n",
    "                    cummulative_p = np.cumsum(p)\n",
    "            \n",
    "                    r = random.random()\n",
    "                    ind = np.where(cummulative_p >= r)[0][0]\n",
    "            \n",
    "                    clusters = np.append(clusters,[self._data[ind]], axis = 0)\n",
    "                self.cluster_centers_ = clusters\n",
    "            else:\n",
    "                raise AttributeError(\"No valid method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict method:\n",
    "used to predict which point depends to a certain cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "        clusters = np.expand_dims(self.cluster_centers_, axis=1)\n",
    "        data = np.expand_dims(X, axis=0)\n",
    "        eucl = np.linalg.norm(clusters-data, axis=2) # euclidean dist by using integrated numpy function\n",
    "        self.labels_ = np.argmin(eucl, axis = 0)\n",
    "        return self.labels_ #returns the cluster with minimum distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform method\n",
    "\n",
    "transforms data to cluster-distance space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(self, X):\n",
    "        clusters = np.expand_dims(self.cluster_centers_, axis=1)\n",
    "        data = np.expand_dims(X, axis=0)\n",
    "        eucl = np.linalg.norm(clusters-data, axis=2)\n",
    "        return eucl.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class MiniBatch Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class MiniBatch Kmeans() takes following arguments:    \n",
    "     **inits** --> initialisations  \n",
    "     Standard = 10    \n",
    "     **.k** --> number of clusters  \n",
    "     Standard = 8  \n",
    "     **max_iterations** --> maximum iterations.  \n",
    "     Standard = 300  \n",
    "     **tol** --> (tolerated percantage of samples changing cluster after one iteration.  \n",
    "     Standard = 1e-3  \n",
    "     **batch_size** --> number of random samples of the data for the Mini Batch.  \n",
    "     Standard = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchKMeans:  # Input: processed dataset, Output: clustered data (MiniBatchKMeans)           \n",
    "    def __init__(self, k=8, inits=300, max_iterations=300, tol=1e-3, batch_size=128):\n",
    "          self._k = k\n",
    "        self._inits = inits\n",
    "        self._max_iterations = max_iterations\n",
    "        self._tol = tol\n",
    "        self._batch_size = batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create_batch method:\n",
    "\n",
    "chooses x (x= batch_size) random data points from data, to create the data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def create_batch(self, data): \n",
    "        data_batch = np.random.choice(range(len(data)), self._batch_size, replace=False)\n",
    "        return data[data_batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initalize method:\n",
    "\n",
    "chooses k random data points from data, to set centers for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   def initialize(self, data): \n",
    "        indices = np.random.choice(range(len(data)), self._k, replace=False)\n",
    "        return data[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### expectation method:\n",
    "\n",
    "measures the euclidean distance between each data_batch points and center points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def expectation(self, data, centroids): \n",
    "        centroids = np.expand_dims(centroids, axis=1)\n",
    "        data_batch = np.expand_dims(data, axis=0)\n",
    "        metric = np.linalg.norm(centroids - data_batch, axis=2)# euclidean dist by using integrated numpy function\n",
    "        return np.argmin(metric, axis=0)#ordnet die data_batch Punkte den centroids zu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### maximization method:\n",
    "\n",
    "For every assigned data point of the batch it moves the centroid in direction of the new barycentre according to the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    @staticmethod\n",
    "    @numba.jit(nopython=True)\n",
    "    def _maximization_aux(data, assignments, centroids, centroid_count): \n",
    "        update = centroids.copy()\n",
    "        for idx, assignment in enumerate(assignments):\n",
    "            data_point = data[idx]\n",
    "            centroid_count[assignment] += 1\n",
    "            lr = 1 / centroid_count[assignment] #learning rate, increases with centroid counts\n",
    "            update[assignment] = update[assignment] * (1 - lr) + data_point * lr\n",
    "        return update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part applies maximization_aux on the data using maximization_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def maximization(self, data, assignments, centroids, centroid_count): \n",
    "        return MiniBatchKMeans._maximization_aux(data, assignments, centroids, centroid_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### final_assignments method:\n",
    "\n",
    "Assignes the rest of the data points to the centroids, which were determined before (not only batch_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def final_assignments(self, data, centroids): \n",
    "        assignments = []\n",
    "        for idx in range(len(data) // self._batch_size + 1):\n",
    "            start = idx * self._batch_size\n",
    "            stop = min(idx * self._batch_size + self._batch_size, len(data))\n",
    "            sub_result = self.expectation(data[start:stop], centroids)\n",
    "            assignments.append(sub_result)\n",
    "        return np.concatenate(assignments, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fit method:\n",
    "\n",
    "fits the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def fit(self, data): \n",
    "        centroids, counts = self.initialize(data)\n",
    "        \n",
    "        old_centroids = None\n",
    "        for idx in range(self._max_iterations):\n",
    "            old_centroids = centroids.copy()\n",
    "            \n",
    "            batch = self.create_batch(data)\n",
    "            assignments = self.expectation(batch, centroids)\n",
    "            centroids = self.maximization(batch, assignments, centroids, counts)\n",
    "            \n",
    "            if np.linalg.norm(centroids - old_centroids) < self._tol: #stops when smaller than tolerance\n",
    "                break\n",
    "\n",
    "        result = self.final_assignments(data, centroids)\n",
    "                \n",
    "        return centroids, result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
